{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fadeb107",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-15 20:56:58.957635: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from xgboost import XGBClassifier # Needed for loading XGBoost\n",
    "from includes.technical_indicators import *\n",
    "from includes.helpers import get_crypto_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c8a6d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 1. Define Indicator Functions (Must be available to the class) ---\n",
    "# (Ensure your calculate_rsi, calculate_macd, etc., are defined here or imported)\n",
    "\n",
    "class CryptoPredictor:\n",
    "    def __init__(self, prefix=\"btc_lstm\"):\n",
    "        self.prefix = prefix\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.threshold = 0.5\n",
    "        self.window_size = 10 \n",
    "        self.model_type = \"keras\"\n",
    "        \n",
    "    def load_artifacts(self):\n",
    "        # \"\"\"Smart loader: Detects if model is Keras, XGBoost, or Pickle.\"\"\"\n",
    "        # print(f\"Loading system for prefix: {self.prefix}...\")\n",
    "        \n",
    "        folder = \"saved_artifacts\"\n",
    "        path_keras = f\"{folder}/{self.prefix}_model.keras\"\n",
    "        path_json  = f\"{folder}/{self.prefix}_model.json\"\n",
    "        path_pkl   = f\"{folder}/{self.prefix}_model.pkl\"\n",
    "        \n",
    "        if os.path.exists(path_keras):\n",
    "            self.model = tf.keras.models.load_model(path_keras)\n",
    "            self.model_type = \"keras\"\n",
    "        elif os.path.exists(path_json):\n",
    "            self.model = XGBClassifier()\n",
    "            self.model.load_model(path_json)\n",
    "            self.model_type = \"xgboost\"\n",
    "        elif os.path.exists(path_pkl):\n",
    "            with open(path_pkl, \"rb\") as f:\n",
    "                self.model = pickle.load(f)\n",
    "            self.model_type = \"sklearn\"\n",
    "        else:\n",
    "            raise ValueError(f\"No model found for prefix '{self.prefix}'\")\n",
    "\n",
    "        with open(f\"{folder}/{self.prefix}_scaler.pkl\", \"rb\") as f:\n",
    "            self.scaler = pickle.load(f)\n",
    "            \n",
    "        with open(f\"{folder}/{self.prefix}_config.json\", \"r\") as f:\n",
    "            config = json.load(f)\n",
    "            self.threshold = config[\"optimal_threshold\"]\n",
    "\n",
    "    def fetch_recent_data(self, ticker=\"BTC-USD\", target_date_str=None):\n",
    "        \"\"\"\n",
    "        Universal Data Fetcher using 'requests'.\n",
    "        Bypasses 'ImpersonateError' from yfinance.\n",
    "        \"\"\"\n",
    "        if target_date_str:\n",
    "            target_dt = datetime.strptime(target_date_str, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            target_dt = datetime.now()\n",
    "\n",
    "        # End: 23:59:59 of target date\n",
    "        end_dt = target_dt.replace(hour=23, minute=59, second=59)\n",
    "        period2 = int(end_dt.timestamp())\n",
    "        \n",
    "        # Start: 180 days lookback (plenty for indicators)\n",
    "        start_dt = end_dt - timedelta(days=180)\n",
    "        period1 = int(start_dt.timestamp())\n",
    "\n",
    "        # Direct Yahoo JSON API URL\n",
    "        url = f\"https://query1.finance.yahoo.com/v8/finance/chart/{ticker}\"\n",
    "        params = {\n",
    "            \"period1\": period1,\n",
    "            \"period2\": period2,\n",
    "            \"interval\": \"1d\",\n",
    "            \"events\": \"div,split\"\n",
    "        }\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, params=params, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            result = data['chart']['result'][0]\n",
    "            timestamps = result['timestamp']\n",
    "            quote = result['indicators']['quote'][0]\n",
    "            \n",
    "            df = pd.DataFrame({\n",
    "                'Date': pd.to_datetime(timestamps, unit='s'),\n",
    "                'Open': quote['open'],\n",
    "                'High': quote['high'],\n",
    "                'Low': quote['low'],\n",
    "                'Close': quote['close'],\n",
    "                'Volume': quote['volume']\n",
    "            })\n",
    "            df.set_index('Date', inplace=True)\n",
    "            # Normalize index to remove time (midnight)\n",
    "            df.index = df.index.normalize() \n",
    "            df = df.dropna()\n",
    "            \n",
    "            if df.empty:\n",
    "                raise ValueError(f\"Data for {ticker} is empty.\")\n",
    "                \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            # We raise the error now so you know downloading failed\n",
    "            raise ValueError(f\"‚ùå FAILED to download {ticker}: {e}\")\n",
    "\n",
    "    def preprocess_live_data(self, df):\n",
    "        \"\"\"Calculates indicators and fetches macros using the working API logic.\"\"\"\n",
    "        # 1. Calculate Technical Indicators\n",
    "        df['RSI'] = calculate_rsi(df)\n",
    "        df['MACD_Line'], df['Signal_Line'] = calculate_macd(df)\n",
    "        df['Williams_R'] = calculate_williams_r(df)\n",
    "        df['Bollinger_B'] = calculate_bollinger_b(df)\n",
    "        df['NATR'] = calculate_natr(df)\n",
    "        df['Vol_ROC'] = calculate_volume_roc(df)\n",
    "        df['Vol_Ratio'] = calculate_vol_ratio(df, period=14)\n",
    "\n",
    "        # 2. --- CORRECTED MACRO FETCHING ---\n",
    "        # Get the latest date from the current BTC dataframe\n",
    "        last_date_str = df.index[-1].strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Map: Feature Name -> Ticker Symbol\n",
    "        macro_map = {\n",
    "            'DXI': 'DX-Y.NYB',\n",
    "            'SP500': '^GSPC',\n",
    "            'GOLD': 'GC=F',\n",
    "            'Ten_Y': '^TNX'\n",
    "        }\n",
    "\n",
    "        # print(f\"   Fetching macro data for {last_date_str}...\")\n",
    "\n",
    "        for col_name, ticker in macro_map.items():\n",
    "            # Use self.fetch_recent_data (which uses requests)\n",
    "            # This ensures we get real data, or crash if impossible (no zeros!)\n",
    "            macro_df = self.fetch_recent_data(ticker, target_date_str=last_date_str)\n",
    "            \n",
    "            # Extract only the Close column and rename it\n",
    "            macro_series = macro_df['Close'].rename(col_name)\n",
    "            \n",
    "            # Merge into BTC dataframe\n",
    "            df = df.join(macro_series, how='left')\n",
    "            \n",
    "            # Forward Fill: Essential because stocks don't trade on weekends\n",
    "            df[col_name] = df[col_name].ffill()\n",
    "            \n",
    "            # Backward Fill: Catch any tiny gaps at the start of the window\n",
    "            df[col_name] = df[col_name].bfill()\n",
    "\n",
    "        # 3. Drop NaNs (Should only happen if macro fetch completely failed/returned nothing)\n",
    "        df = df.dropna()\n",
    "        \n",
    "        # 4. Feature Selection\n",
    "        feature_cols =['Open', 'High', 'Low', 'Close', 'Volume', \n",
    "                       'DXI', 'SP500', 'GOLD', 'Ten_Y', \n",
    "                       'RSI', 'MACD_Line', 'Signal_Line', 'Williams_R', \n",
    "                       'Bollinger_B', 'NATR', 'Vol_ROC', 'Vol_Ratio']\n",
    "        \n",
    "        # Double check we have all columns (Safety)\n",
    "        missing_cols = [c for c in feature_cols if c not in df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing columns after processing: {missing_cols}\")\n",
    "\n",
    "        # 5. Get the LAST Window\n",
    "        last_window = df[feature_cols].tail(self.window_size).values\n",
    "        \n",
    "        if len(last_window) < self.window_size:\n",
    "            raise ValueError(f\"Not enough data. Needed {self.window_size}, got {len(last_window)}.\")\n",
    "            \n",
    "        # 6. Scale\n",
    "        scaled_window = self.scaler.transform(last_window)\n",
    "        \n",
    "        # 7. Reshape\n",
    "        if self.model_type == \"keras\":\n",
    "            final_input = np.expand_dims(scaled_window, axis=0)\n",
    "        else:\n",
    "            final_input = scaled_window.flatten().reshape(1, -1)\n",
    "        \n",
    "        return final_input, df['Close'].iloc[-1], df.index[-1]\n",
    "\n",
    "    def predict_next_day(self, target_date):\n",
    "        \"\"\"Fetches data, predicts, and returns stats.\"\"\"\n",
    "        # 1. Fetch BTC Data\n",
    "        df = self.fetch_recent_data(\"BTC-USD\", target_date_str=target_date)\n",
    "        \n",
    "        # 2. Process (will now fetch macros properly)\n",
    "        X_input, current_price, _ = self.preprocess_live_data(df)\n",
    "        \n",
    "        # 3. Predict\n",
    "        if self.model_type == \"keras\":\n",
    "            prob = self.model.predict(X_input, verbose=0)[0][0]\n",
    "        else:\n",
    "            prob = self.model.predict_proba(X_input)[0][1]\n",
    "        \n",
    "        return {\n",
    "            \"Model\": self.prefix.replace(\"btc_\", \"\").upper(),\n",
    "            \"Probability\": prob, \n",
    "            \"Threshold\": self.threshold,\n",
    "            \"Trend\": \"UP üü¢\" if prob > self.threshold else \"DOWN üî¥\",\n",
    "            \"Confidence\": abs(prob - 0.5) * 200\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38dee54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Date:       2025-12-13\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- üèÜ MODEL CONSENSUS TABLE ---\n",
      "  Model Probability Threshold  Trend Confidence\n",
      "   LSTM      63.38%      0.38   UP üü¢     26.75%\n",
      "XGBOOST      50.10%      0.51 DOWN üî¥      0.19%\n",
      "     RF      51.11%      0.48   UP üü¢      2.22%\n",
      "    SVM      51.70%      0.52 DOWN üî¥      3.41%\n",
      "    KNN      50.08%      0.45   UP üü¢      0.15%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "model_prefixes = [\"btc_lstm\", \"btc_xgboost\", \"btc_rf\", \"btc_svm\", \"btc_knn\"]\n",
    "target_date = \"2025-12-13\"\n",
    "\n",
    "print(f\"Data Date:       {target_date}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# 1. Collect results in a list\n",
    "results_list = []\n",
    "\n",
    "for prefix in model_prefixes:\n",
    "    try:\n",
    "        bot = CryptoPredictor(prefix)\n",
    "        bot.load_artifacts()\n",
    "        \n",
    "        # Get dictionary return\n",
    "        stats = bot.predict_next_day(target_date)\n",
    "        results_list.append(stats)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error with {prefix}: {e}\")\n",
    "        results_list.append({\"Model\": prefix, \"Trend\": \"ERROR\"})\n",
    "\n",
    "# 2. Create DataFrame for the table\n",
    "if results_list:\n",
    "    df = pd.DataFrame(results_list)\n",
    "\n",
    "    # 3. Format numbers for cleaner display (Optional)\n",
    "    # Probability: 0.55 -> 55.00%\n",
    "    if 'Probability' in df.columns:\n",
    "        df['Probability'] = df['Probability'].apply(lambda x: f\"{x:.2%}\" if isinstance(x, (float, int)) else x)\n",
    "    \n",
    "    # Confidence: 10.543 -> 10.54%\n",
    "    if 'Confidence' in df.columns:\n",
    "        df['Confidence'] = df['Confidence'].apply(lambda x: f\"{x:.2f}%\" if isinstance(x, (float, int)) else x)\n",
    "\n",
    "    # Threshold: 0.5 -> 0.50\n",
    "    if 'Threshold' in df.columns:\n",
    "        df['Threshold'] = df['Threshold'].apply(lambda x: f\"{x:.2f}\" if isinstance(x, (float, int)) else x)\n",
    "\n",
    "    # 4. Print Table (hide index number for cleaner look)\n",
    "    print(\"\\n--- üèÜ MODEL CONSENSUS TABLE ---\")\n",
    "    print(df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
